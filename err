import threading
import cv2
import speech_recognition as sr
import os
import time
import json
import face_recognition
import numpy as np
import logging
import hashlib
import subprocess
from flask import Flask, render_template, Response
from gtts import gTTS
from ctypes import *
from contextlib import contextmanager

# --- ALSA ERROR SUPPRESSION ---
ERROR_HANDLER_FUNC = CFUNCTYPE(None, c_char_p, c_int, c_char_p, c_int, c_char_p)
def py_error_handler(filename, line, function, err, fmt):
    pass
c_error_handler = ERROR_HANDLER_FUNC(py_error_handler)

@contextmanager
def no_alsa_errors():
    try:
        asound = cdll.LoadLibrary('libasound.so')
        asound.snd_lib_error_set_handler(c_error_handler)
        yield
        asound.snd_lib_error_set_handler(None)
    except:
        yield

# --- CONFIGURATION ---
app = Flask(__name__)
log = logging.getLogger('werkzeug')
log.setLevel(logging.ERROR)

MIC_INDEX = None 

# Global Variables
video_frame = None
known_face_encodings = []
known_face_names = []
current_detected_name = None  
current_status = "SYSTEM READY"

# --- STEP 1: LOAD DATABASE ---
def load_student_data():
    print(" [DB] Loading Student Faces...")
    global known_face_encodings, known_face_names
    
    path = "student_pics"
    if not os.path.exists(path):
        os.makedirs(path)
        print(" [DB] 'student_pics' folder created. Add images there.")
        return

    for file in os.listdir(path):
        if file.endswith((".jpg", ".png", ".jpeg")):
            try:
                img = face_recognition.load_image_file(f"{path}/{file}")
                encodings = face_recognition.face_encodings(img)
                
                if len(encodings) > 0:
                    known_face_encodings.append(encodings[0])
                    name = os.path.splitext(file)[0]
                    known_face_names.append(name)
                    print(f"   + Loaded: {name}")
            except Exception as e:
                print(f"   - Error loading {file}: {e}")
    
    print(f" [DB] System Ready. Known students: {len(known_face_names)}")

# --- DATABASE HELPER ---
def get_student_info(name):
    """Retrieves full student object (registered status and ID)"""
    try:
        if not os.path.exists('students.json'):
            with open('students.json', 'w') as f:
                json.dump({}, f)
                
        with open('students.json', 'r') as f:
            db = json.load(f)
        
        return db.get(name) # Returns None if not found
    except Exception as e:
        print(f" [DB ERROR] {e}")
        return None

# --- AUDIO FUNCTIONS ---
def update_status(status):
    global current_status
    current_status = status

def speak(text):
    global current_status
    print(f"Robot: {text}")
    
    prev_status = current_status
    update_status(f"SPEAKING: {text}")
    
    filename = hashlib.md5(text.encode()).hexdigest() + ".mp3"
    file_path = f"audio_cache/{filename}"
    
    if not os.path.exists("audio_cache"):
        os.makedirs("audio_cache")

    if not os.path.exists(file_path):
        try:
            tts = gTTS(text=text, lang='en', tld='co.uk')
            tts.save(file_path)
        except Exception as e:
            print(f" [ERROR] Could not generate TTS: {e}")
            update_status(prev_status)
            return

    subprocess.run(["mpg123", "-q", file_path])
    update_status(prev_status)

def listen(prompt_text=None):
    """Generic listen function for Name or ID"""
    update_status("LISTENING")
    
    if prompt_text:
        speak(prompt_text)
    
    r = sr.Recognizer()
    
    with no_alsa_errors():
        try:
            mic = sr.Microphone(device_index=MIC_INDEX)
            with mic as source:
                r.adjust_for_ambient_noise(source, duration=0.5)
                print(" [AUDIO] Listening...")
                audio = r.listen(source, timeout=5, phrase_time_limit=5)
                
                update_status("THINKING")
                # Using Sphinx (Offline). Change to recognize_google(audio) if online.
                text = r.recognize_sphinx(audio).lower()
                print(f" [AUDIO] Heard: {text}")
                return text
        except sr.WaitTimeoutError:
            print(" [AUDIO] Timeout.")
            return None
        except sr.UnknownValueError:
            speak("I did not understand.")
            return None
        except Exception as e:
            print(f" [AUDIO] Error: {e}")
            return None
        finally:
            update_status("IDLE")

def clean_id(text):
    """Converts spoken words to digits for ID comparison"""
    if not text: return ""
    
    # Map for Sphinx output (e.g. 'one' -> '1')
    word_map = {
        'zero': '0', 'one': '1', 'two': '2', 'three': '3', 'four': '4',
        'five': '5', 'six': '6', 'seven': '7', 'eight': '8', 'nine': '9'
    }
    
    words = text.split()
    clean_text = ""
    
    for word in words:
        if word in word_map:
            clean_text += word_map[word]
        elif word.isdigit():
            clean_text += word
        else:
            # If they say "ID is 123", we ignore "ID" and "is"
            pass
            
    # Fallback: if map failed, just return lowercase without spaces
    if not clean_text:
        return text.replace(" ", "").lower()
        
    return clean_text

# --- WEB SERVER ---
@app.route('/')
def index():
    return render_template('index.html')

def generate_frames():
    global video_frame
    while True:
        if video_frame is not None:
            try:
                ret, buffer = cv2.imencode('.jpg', video_frame)
                yield (b'--frame\r\nContent-Type: image/jpeg\r\n\r\n' + buffer.tobytes() + b'\r\n')
            except Exception as e:
                pass
        time.sleep(0.04)

@app.route('/video_feed')
def video_feed():
    return Response(generate_frames(), mimetype='multipart/x-mixed-replace; boundary=frame')

@app.route('/status_feed')
def status_feed():
    def generate():
        while True:
            yield f"data: {current_status}\n\n"
            time.sleep(0.5)
    return Response(generate(), mimetype="text/event-stream")

# --- LOGIC LOOP ---
def exam_logic_loop():
    global current_detected_name
    
    time.sleep(3) 
    speak("System Online.")
    
    while True:
        update_status("WAITING FOR FACE")
        
        # 1. Wait for Face
        if current_detected_name is None:
            time.sleep(0.5)
            continue
            
        # 2. Instruct
        update_status("INSTRUCTING STUDENT")
        speak("Hi, Please look directly at the camera. Please remove any things from your face or head for proper detection.")
        time.sleep(3)
        
        if current_detected_name is None:
            speak("Face lost. Process reset.")
            continue
            
        # 3. Process Name
        name = current_detected_name
        update_status(f"PROCESSING: {name}")
        
        # If face is Unknown, ask for name
        if name == "Unknown":
            spoken_name = listen("Face not recognized. Please state your name.")
            if spoken_name:
                speak(f"I heard {spoken_name}.")
                name = spoken_name.replace(" ", "_")
            else:
                current_detected_name = None
                continue
        
        # 4. Retrieve Data
        student_data = get_student_info(name)
        
        if not student_data:
            speak(f"I cannot find any record for {name}.")
            time.sleep(2)
            current_detected_name = None
            continue

        # --- NEW: ID VERIFICATION ---
        stored_id = str(student_data.get('id', ''))
        
        if not stored_id:
            speak(f"Warning. No ID found in database for {name}.")
            # Optional: Allow entry if ID is missing? Or Fail? 
            # Currently just skipping ID check if DB is empty
            pass
        else:
            # Ask for ID
            spoken_text = listen(f"Hello {name}. Please state your Student I D.")
            
            if not spoken_text:
                speak("I did not hear an I D.")
                current_detected_name = None # Reset
                continue
            
            # Compare
            clean_spoken = clean_id(spoken_text)
            clean_stored = clean_id(stored_id)
            
            # lenient check: if spoken is in stored or stored in spoken
            if clean_spoken == clean_stored or clean_stored in clean_spoken:
                speak("Identity confirmed.")
            else:
                speak(f"Identity mismatch. I heard {clean_spoken}, but expected {clean_stored}.")
                time.sleep(1)
                speak("Access Denied.")
                # Wait for user to leave
                while current_detected_name is not None: time.sleep(1)
                continue

        # 5. Registration Check
        is_registered = student_data.get('registered', False)
        
        if is_registered:
            speak("You are registered.")
            
            # Exam Rules
            update_status("READING RULES")
            speak("Attention. Please listen to the exam rules.")
            time.sleep(0.2)
            speak("No phones or smart watches are allowed in the exam hall or during the exam.")
            time.sleep(0.2)
            speak("Failure to follow these rules will result in the dismissal of the test.")
            time.sleep(0.2)
            speak("You may enter now.")
            
            while current_detected_name == name: time.sleep(1)
            speak("Next student.")
            
        else:
            speak(f"Alert. {name}, you are NOT registered.")
            while current_detected_name == name: time.sleep(1)

        time.sleep(1)

# --- VISION LOOP ---
def vision_loop():
    global video_frame, current_detected_name
    
    print(" [VISION] Starting Camera...")
    
    # Safe pipeline for RPi 5/Zero2W with libcamerasrc
    pipelines = [
        "libcamerasrc ! video/x-raw, width=640, height=480, framerate=15/1, format=YUY2 ! videoconvert ! video/x-raw, format=BGR ! appsink drop=true max-buffers=1 sync=false",
        0
    ]
    
    cap = None
    for pipeline in pipelines:
        if isinstance(pipeline, str):
            cap = cv2.VideoCapture(pipeline, cv2.CAP_GSTREAMER)
        else:
            cap = cv2.VideoCapture(pipeline)
        if cap.isOpened():
            break
            
    if not cap or not cap.isOpened():
        print(" [ERROR] No camera found.")
        return

    frame_count = 0
    cached_face_locations = []
    cached_face_names = []

    while True:
        try:
            ret, frame = cap.read()
            if not ret:
                time.sleep(0.1)
                continue
            
            frame_count += 1
            if frame_count % 5 == 0:
                small = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)
                rgb = cv2.cvtColor(small, cv2.COLOR_BGR2RGB)
                cached_face_locations = face_recognition.face_locations(rgb)
                encodings = face_recognition.face_encodings(rgb, cached_face_locations)
                
                cached_face_names = []
                detected = None
                if encodings:
                    for enc in encodings:
                        name = "Unknown"
                        if len(known_face_encodings) > 0:
                            matches = face_recognition.compare_faces(known_face_encodings, enc)
                            dists = face_recognition.face_distance(known_face_encodings, enc)
                            if len(dists) > 0:
                                best = np.argmin(dists)
                                if matches[best]: name = known_face_names[best]
                        cached_face_names.append(name)
                        detected = name
                current_detected_name = detected

            for (t, r, b, l), name in zip(cached_face_locations, cached_face_names):
                t*=4; r*=4; b*=4; l*=4
                color = (0, 255, 0) if name != "Unknown" else (0, 0, 255)
                cv2.rectangle(frame, (l, t), (r, b), color, 2)
                cv2.rectangle(frame, (l, b-35), (r, b), color, cv2.FILLED)
                cv2.putText(frame, str(name), (l+6, b-6), cv2.FONT_HERSHEY_DUPLEX, 0.8, (255,255,255), 1)
            video_frame = frame
        except Exception:
            time.sleep(0.1)

if __name__ == "__main__":
    load_student_data()
    threading.Thread(target=vision_loop, daemon=True).start()
    threading.Thread(target=exam_logic_loop, daemon=True).start()
    app.run(host='0.0.0.0', port=5000, debug=False, use_reloader=False)
